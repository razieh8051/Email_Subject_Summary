{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a seq_to_seq problem. (needs encoder and decoder)<br>\n",
    "Process:<br>\n",
    "1: Load the data <br>\n",
    "2: Preprocess the data <br>\n",
    "3: Encode the sentences (create the dictionary from words, map words to integers) <br>\n",
    "4: Build and train the seq2seq model (Using GloVe for the embeddings and Attention with decoder) <br> \n",
    "5: Generate the summary (Subjects for email contents) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tensorflow.python.layers.core import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from Enron email dataset. <br>\n",
    "In this case we consider the subject of an email a few words summary that we need to learn for that email. Therefore, the inputs are email contents and targets are email subjects (or summaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/emails_data/enron_emails.csv')\n",
    "#Extracting only two columns 'Subject' and 'content'\n",
    "df1 = df[['Subject','content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We disregard the forwarded and replied emails Since they do not provide any particular subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=df1[~df1['Subject'].str.contains(\"FW:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"Fw:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"fw:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"RE:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"Re:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"re:\", na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emails that contain \"Forwarded by\" in their content are the replied emails and their subjects are changed by the current sender. Therefore, for now we disregard those too. (Although later we can use the replied emails for the test part to give them subjects.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=df1[~df1['content'].str.contains(\"Forwarded by\", na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NaN subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>San Juan Index</td>\n",
       "      <td>Liane, As we discussed yesterday, I am concern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>tv on 33</td>\n",
       "      <td>Cash Hehub Chicago PEPL Katy Socal Opal Permia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>For Wade</td>\n",
       "      <td>Wade, I understood your number one priority wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>assoc. for west desk</td>\n",
       "      <td>Celeste, I need two assoc./analyst for the wes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>test</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Priority List</td>\n",
       "      <td>Will, Here is a list of the top items we need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>eol</td>\n",
       "      <td>Jeff/Brenda: Please authorize the following pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Mike Grigsby</td>\n",
       "      <td>Please approve Mike Grigsby for Bloomberg. Tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>San Marcos construction project</td>\n",
       "      <td>Please find attached the pro formas for the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Headcount</td>\n",
       "      <td>Financial (6) West Desk (14) Mid Market (16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>updated lease information</td>\n",
       "      <td>Lucy, The apartments that have new tenants sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Enron</td>\n",
       "      <td>Jed, I understand you have been contacted rega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>December 14, 2000 - Bear Stearns' predictions ...</td>\n",
       "      <td>In today's Daily Update you'll find free repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>Bloomberg Power Lines Report</td>\n",
       "      <td>Here is today's copy of Bloomberg Power Lines....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>Special report coming from NewsData</td>\n",
       "      <td>Our Sacramento correspondent just exited a new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>San Juan Index</td>\n",
       "      <td>Liane, As we discussed yesterday, I am concern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>New Notice from Transwestern Pipeline Co.</td>\n",
       "      <td>Transwestern Pipeline Co. posted new notice(s)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>MARKET ALERT: Nasdaq Composite Ends Down 3.7%</td>\n",
       "      <td>MARKET ALERT from The Wall Street Journal Dece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>FS Van Kasper Initiates Coverage of NT</td>\n",
       "      <td>If you cannot read this email, please click he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>tv on 33</td>\n",
       "      <td>Cash Hehub Chicago PEPL Katy Socal Opal Permia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Y-Life Daily: Bush will almost definitely be p...</td>\n",
       "      <td>Y-Life Daily Bulletin: December 13, 2000 Note:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>December Newsletter - Factory Incentives are a...</td>\n",
       "      <td>As requested, here is the December Autoweb.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>Western Price Survey 12/13/2000</td>\n",
       "      <td>I'm sending this early because we expect every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>systems wish list</td>\n",
       "      <td>attached is the systems wish list for the gas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>For Wade</td>\n",
       "      <td>Wade, I understood your number one priority wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>assoc. for west desk</td>\n",
       "      <td>Celeste, I need two assoc./analyst for the wes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>test</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>New Notice from Transwestern Pipeline Co.</td>\n",
       "      <td>Transwestern Pipeline Co. posted new notice(s)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>Ken Lay and Jeff Skilling on CNNfn</td>\n",
       "      <td>Ken Lay and Jeff Skilling were interviewed on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>New Notice from Transwestern Pipeline Co.</td>\n",
       "      <td>Transwestern Pipeline Co. posted new notice(s)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517248</th>\n",
       "      <td>sign-on</td>\n",
       "      <td>one thing, we should wait before we dole out a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517253</th>\n",
       "      <td>Jason Garvey</td>\n",
       "      <td>I recognize that Aug 17 is Jason Garvey's last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517260</th>\n",
       "      <td>EOL Trade Volume</td>\n",
       "      <td>From the beginning of the year to July 25 No o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517263</th>\n",
       "      <td>Natural Gas Reports</td>\n",
       "      <td>Thanks for making the changes on the P&amp;L and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517267</th>\n",
       "      <td>Natural gas executive reports</td>\n",
       "      <td>Can you change John MacKay's name in the Natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517287</th>\n",
       "      <td>2nd Order Changes</td>\n",
       "      <td>John, I lost the following on 2nd order in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517293</th>\n",
       "      <td>Alberta Power vs MID-C</td>\n",
       "      <td>I sent you an update of the Hedge Summary for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517294</th>\n",
       "      <td>Hedge Summary July 3,01</td>\n",
       "      <td>Here is the hedge summary for July 3/01. Two t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517319</th>\n",
       "      <td>Contini Leadbeater Personal Guarantee</td>\n",
       "      <td>Here are my comments on the original guarantee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517323</th>\n",
       "      <td>Beginning of year power marks</td>\n",
       "      <td>Here is the spreadsheet that I sent you at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517325</th>\n",
       "      <td>hedge summary</td>\n",
       "      <td>If you look at the Cal 03-Cal 10 period, you s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517329</th>\n",
       "      <td>computer eligibility</td>\n",
       "      <td>I was wondering why I am not eligible yet. I s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517332</th>\n",
       "      <td>Meeting with Fallon today</td>\n",
       "      <td>can we possibly move this meeting up from 3:30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517335</th>\n",
       "      <td>Canadian function currency</td>\n",
       "      <td>We have traditionally have had currency issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517341</th>\n",
       "      <td>Trades</td>\n",
       "      <td>Can you send me the Jan 18 trades and marks as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517344</th>\n",
       "      <td>Ontario</td>\n",
       "      <td>can you prepare how the ontario market works p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517350</th>\n",
       "      <td>Trades and curve</td>\n",
       "      <td>Can you send my curves and trades for Jan 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517351</th>\n",
       "      <td>meeting</td>\n",
       "      <td>Can Dorland and Chad meet me and Mili in Video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517352</th>\n",
       "      <td>Functional Currency</td>\n",
       "      <td>After some brief discussions with UBS in Houst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517357</th>\n",
       "      <td>1998 Silver C4S</td>\n",
       "      <td>Check out www.segalmotorcar.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517363</th>\n",
       "      <td>Lotus Elise</td>\n",
       "      <td>Is is possible to import a lotus elise and get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517366</th>\n",
       "      <td>Products</td>\n",
       "      <td>Nella here is a list of products that we would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517367</th>\n",
       "      <td>Chevron Contract</td>\n",
       "      <td>Enron Canada Power Corp acknowledges the chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517372</th>\n",
       "      <td>Training</td>\n",
       "      <td>I am interested in a program for myself. I am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517375</th>\n",
       "      <td>Lethbrige Ironworks</td>\n",
       "      <td>you can increase the direct sale to include De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517378</th>\n",
       "      <td>Gas and Power EOL Access</td>\n",
       "      <td>Here is the update list that you requested. Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517384</th>\n",
       "      <td>Digital Certificate for TAU</td>\n",
       "      <td>Please set up access for the digital certifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517396</th>\n",
       "      <td>Trade with John Lavorato</td>\n",
       "      <td>This is a trade with OIL-SPEC-HEDGE-NG (John L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517397</th>\n",
       "      <td>Gas Hedges</td>\n",
       "      <td>Some of my position is with the Alberta Term b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517399</th>\n",
       "      <td>Calgary Analyst/Associate</td>\n",
       "      <td>Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243594 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Subject  \\\n",
       "24                                         San Juan Index   \n",
       "106                                              tv on 33   \n",
       "126                                              For Wade   \n",
       "140                                  assoc. for west desk   \n",
       "143                                                  test   \n",
       "224                                         Priority List   \n",
       "267                                                   eol   \n",
       "395                                          Mike Grigsby   \n",
       "413                       San Marcos construction project   \n",
       "518                                             Headcount   \n",
       "519                             updated lease information   \n",
       "599                                                 Enron   \n",
       "602     December 14, 2000 - Bear Stearns' predictions ...   \n",
       "603                          Bloomberg Power Lines Report   \n",
       "614                   Special report coming from NewsData   \n",
       "616                                        San Juan Index   \n",
       "645             New Notice from Transwestern Pipeline Co.   \n",
       "656         MARKET ALERT: Nasdaq Composite Ends Down 3.7%   \n",
       "678                FS Van Kasper Initiates Coverage of NT   \n",
       "696                                              tv on 33   \n",
       "700     Y-Life Daily: Bush will almost definitely be p...   \n",
       "711     December Newsletter - Factory Incentives are a...   \n",
       "712                       Western Price Survey 12/13/2000   \n",
       "716                                     systems wish list   \n",
       "720                                              For Wade   \n",
       "733                                  assoc. for west desk   \n",
       "736                                                  test   \n",
       "744             New Notice from Transwestern Pipeline Co.   \n",
       "755                    Ken Lay and Jeff Skilling on CNNfn   \n",
       "766             New Notice from Transwestern Pipeline Co.   \n",
       "...                                                   ...   \n",
       "517248                                            sign-on   \n",
       "517253                                       Jason Garvey   \n",
       "517260                                   EOL Trade Volume   \n",
       "517263                                Natural Gas Reports   \n",
       "517267                      Natural gas executive reports   \n",
       "517287                                  2nd Order Changes   \n",
       "517293                             Alberta Power vs MID-C   \n",
       "517294                            Hedge Summary July 3,01   \n",
       "517319              Contini Leadbeater Personal Guarantee   \n",
       "517323                      Beginning of year power marks   \n",
       "517325                                      hedge summary   \n",
       "517329                               computer eligibility   \n",
       "517332                          Meeting with Fallon today   \n",
       "517335                         Canadian function currency   \n",
       "517341                                             Trades   \n",
       "517344                                            Ontario   \n",
       "517350                                   Trades and curve   \n",
       "517351                                            meeting   \n",
       "517352                                Functional Currency   \n",
       "517357                                    1998 Silver C4S   \n",
       "517363                                        Lotus Elise   \n",
       "517366                                           Products   \n",
       "517367                                   Chevron Contract   \n",
       "517372                                           Training   \n",
       "517375                                Lethbrige Ironworks   \n",
       "517378                           Gas and Power EOL Access   \n",
       "517384                        Digital Certificate for TAU   \n",
       "517396                           Trade with John Lavorato   \n",
       "517397                                         Gas Hedges   \n",
       "517399                          Calgary Analyst/Associate   \n",
       "\n",
       "                                                  content  \n",
       "24      Liane, As we discussed yesterday, I am concern...  \n",
       "106     Cash Hehub Chicago PEPL Katy Socal Opal Permia...  \n",
       "126     Wade, I understood your number one priority wa...  \n",
       "140     Celeste, I need two assoc./analyst for the wes...  \n",
       "143                                               testing  \n",
       "224     Will, Here is a list of the top items we need ...  \n",
       "267     Jeff/Brenda: Please authorize the following pr...  \n",
       "395     Please approve Mike Grigsby for Bloomberg. Tha...  \n",
       "413     Please find attached the pro formas for the pr...  \n",
       "518          Financial (6) West Desk (14) Mid Market (16)  \n",
       "519     Lucy, The apartments that have new tenants sin...  \n",
       "599     Jed, I understand you have been contacted rega...  \n",
       "602     In today's Daily Update you'll find free repor...  \n",
       "603     Here is today's copy of Bloomberg Power Lines....  \n",
       "614     Our Sacramento correspondent just exited a new...  \n",
       "616     Liane, As we discussed yesterday, I am concern...  \n",
       "645     Transwestern Pipeline Co. posted new notice(s)...  \n",
       "656     MARKET ALERT from The Wall Street Journal Dece...  \n",
       "678     If you cannot read this email, please click he...  \n",
       "696     Cash Hehub Chicago PEPL Katy Socal Opal Permia...  \n",
       "700     Y-Life Daily Bulletin: December 13, 2000 Note:...  \n",
       "711     As requested, here is the December Autoweb.com...  \n",
       "712     I'm sending this early because we expect every...  \n",
       "716     attached is the systems wish list for the gas ...  \n",
       "720     Wade, I understood your number one priority wa...  \n",
       "733     Celeste, I need two assoc./analyst for the wes...  \n",
       "736                                               testing  \n",
       "744     Transwestern Pipeline Co. posted new notice(s)...  \n",
       "755     Ken Lay and Jeff Skilling were interviewed on ...  \n",
       "766     Transwestern Pipeline Co. posted new notice(s)...  \n",
       "...                                                   ...  \n",
       "517248  one thing, we should wait before we dole out a...  \n",
       "517253  I recognize that Aug 17 is Jason Garvey's last...  \n",
       "517260  From the beginning of the year to July 25 No o...  \n",
       "517263  Thanks for making the changes on the P&L and f...  \n",
       "517267  Can you change John MacKay's name in the Natur...  \n",
       "517287  John, I lost the following on 2nd order in the...  \n",
       "517293  I sent you an update of the Hedge Summary for ...  \n",
       "517294  Here is the hedge summary for July 3/01. Two t...  \n",
       "517319  Here are my comments on the original guarantee...  \n",
       "517323  Here is the spreadsheet that I sent you at the...  \n",
       "517325  If you look at the Cal 03-Cal 10 period, you s...  \n",
       "517329  I was wondering why I am not eligible yet. I s...  \n",
       "517332  can we possibly move this meeting up from 3:30...  \n",
       "517335  We have traditionally have had currency issues...  \n",
       "517341  Can you send me the Jan 18 trades and marks as...  \n",
       "517344  can you prepare how the ontario market works p...  \n",
       "517350       Can you send my curves and trades for Jan 18  \n",
       "517351  Can Dorland and Chad meet me and Mili in Video...  \n",
       "517352  After some brief discussions with UBS in Houst...  \n",
       "517357                    Check out www.segalmotorcar.com  \n",
       "517363  Is is possible to import a lotus elise and get...  \n",
       "517366  Nella here is a list of products that we would...  \n",
       "517367  Enron Canada Power Corp acknowledges the chang...  \n",
       "517372  I am interested in a program for myself. I am ...  \n",
       "517375  you can increase the direct sale to include De...  \n",
       "517378  Here is the update list that you requested. Mi...  \n",
       "517384  Please set up access for the digital certifica...  \n",
       "517396  This is a trade with OIL-SPEC-HEDGE-NG (John L...  \n",
       "517397  Some of my position is with the Alberta Term b...  \n",
       "517399  Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...  \n",
       "\n",
       "[243594 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1[pd.notnull(df1['Subject'])]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out the maximum and minimum length for the content column so we can define a specific length range for emails that we want to include in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject       258\n",
      "content    737640\n",
      "dtype: int64\n",
      "Subject    1\n",
      "content    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "max_len = df1.applymap(lambda x: len(str(x))).max()\n",
    "print(max_len)\n",
    "min_len = df1.applymap(lambda x: len(str(x))).min()\n",
    "print(min_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only include the emails with the contents in the range of [500,6000] characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = (0<df1['Subject'].str.len()<258) & (500<df1['content'].str.len() <6000)\n",
    "#df1 = df1.loc[mask]\n",
    "df1=df1[df1['content'].astype('str').map(len) <= 6000]\n",
    "df1=df1[df1['content'].astype('str').map(len) >= 500] \n",
    "emails=df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean emails contents and subjects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean(emails,stop_words):\n",
    "    '''Clean the data both subjects and content of emails'''\n",
    "    emails_messages=[]\n",
    "    subj_messages=[]\n",
    "    for email_content in emails['content']:\n",
    "        #Extra celaning of text before tokenization \n",
    "        #Removing stopwords                \n",
    "        email_content=' '.join(i for i in email_content.split() if i not in stop_words)\n",
    "        #Removing special characters and float numbers\n",
    "        email_content=re.sub(\"(\\d*\\.\\d+)|(\\d+\\.[0-9 ]+)\",\"\",email_content)\n",
    "        email_content=re.sub(r'[^\\w]', ' ', email_content)\n",
    "        '''for word in email_content:\n",
    "            email_content=\" \".join([w for w in email_content.split() if not w.isdigit()])'''\n",
    "        #Removing all numbers (except for joint numbers to strings such as 27th; we also may later try to keep numbers related to dates and rooms, money , etc such as Sep 27, room numbers 3, 10 cent, etc)\n",
    "        email_content = \" \".join([w for w in email_content.split() if not w.isdigit()])\n",
    "\n",
    "        emails_messages.append(email_content)\n",
    "    for subject_messages in emails['Subject']:\n",
    "        #Extra celaning of text before tokenization \n",
    "        #Removing stopwords                \n",
    "        subject_messages=' '.join(i for i in subject_messages.split() if i not in stop_words)\n",
    "        #Removing special characters and float numbers\n",
    "        subject_messages=re.sub(\"(\\d*\\.\\d+)|(\\d+\\.[0-9 ]+)\",\"\",subject_messages)\n",
    "        subject_messages=re.sub(r'[^\\w]', ' ', subject_messages)\n",
    "        '''for word in email_content:\n",
    "            email_content=\" \".join([w for w in email_content.split() if not w.isdigit()])'''\n",
    "        #Removing all numbers (except for joint numbers to strings such as 27th; we also may later try to keep numbers related to dates and rooms, money , etc such as Sep 27, room numbers 3, 10 cent, etc)\n",
    "        subject_messages = \" \".join([w for w in subject_messages.split() if not w.isdigit()])\n",
    "\n",
    "        subj_messages.append(subject_messages)    \n",
    "\n",
    "    return subj_messages,emails_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Loading stop words from nltk\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#Clean and preprocess the data\n",
    "subject_messages,emails_messages=load_clean(emails,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('First email before:', \"Liane, As we discussed yesterday, I am concerned there has been an attempt to manipulate the El Paso San Juan monthly index. A single buyer entered the marketplace on both September 26 and 27 and paid above market prices ($4.70-$4.80) for San Juan gas with the intent to distort the index. At the time of these trades, offers for physical gas at significantly (10 to 15 cents) lower prices were bypassed in order to establish higher trades to report into the index calculation. Additionally, these trades are out of line with the associated financial swaps for San Juan. We have compiled a list of financial and physical trades executed from September 25 to September 27. These are the complete list of trades from Enron Online (EOL), Enron's direct phone conversations, and three brokerage firms (Amerex, APB, and Prebon). Please see the attached spreadsheet for a trade by trade list and a summary. We have also included a summary of gas daily prices to illustrate the value of San Juan based on several spread relationships. The two key points from this data are as follows: 1. The high physical prices on the 26th & 27th (4.75,4,80) are much greater than the high financial trades (4.6375,4.665) on those days. 2. The spread relationship between San Juan and other points (Socal & Northwest) is consistent between the end of September and October gas daily. It doesn't make sense to have monthly indeces that are dramatically different. I understand you review the trades submitted for outliers. Hopefully, the trades submitted will reveal counterparty names and you will be able to determine that there was only one buyer in the 4.70's and these trades are outliers. I wanted to give you some additional points of reference to aid in establishing a reasonable index. It is Enron's belief that the trades at $4.70 and higher were above market trades that should be excluded from the calculation of index. It is our desire to have reliable and accurate indeces against which to conduct our physical and financial business. Please contact me anytime I can assist you towards this goal. Sincerely, Phillip Allen\")\n",
      "('First email after:', 'Liane As discussed yesterday I concerned attempt manipulate El Paso San Juan monthly index A single buyer entered marketplace September paid market prices San Juan gas intent distort index At time trades offers physical gas significantly cents lower prices bypassed order establish higher trades report index calculation Additionally trades line associated financial swaps San Juan We compiled list financial physical trades executed September September These complete list trades Enron Online EOL Enron s direct phone conversations three brokerage firms Amerex APB Prebon Please see attached spreadsheet trade trade list summary We also included summary gas daily prices illustrate value San Juan based several spread relationships The two key points data follows The high physical prices 26th 27th much greater high financial trades days The spread relationship San Juan points Socal Northwest consistent end September October gas daily It make sense monthly indeces dramatically different I understand review trades submitted outliers Hopefully trades submitted reveal counterparty names able determine one buyer s trades outliers I wanted give additional points reference aid establishing reasonable index It Enron s belief trades higher market trades excluded calculation index It desire reliable accurate indeces conduct physical financial business Please contact anytime I assist towards goal Sincerely Phillip Allen')\n"
     ]
    }
   ],
   "source": [
    "#Comapring the first email before and after preprocessing\n",
    "print(\"First email before:\",emails.loc[24,'content'])\n",
    "\n",
    "print(\"First email after:\",emails_messages[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<EOS> specifies the end of text<br>\n",
    "<SOS> specifies the beggning of each sequence as well as each batch <br>\n",
    "<PAD> is used to make sequences to have the same lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words_in_seqences(phrases,sentences):\n",
    "    '''Convert words to numbers (Create the dictionary of words)'''\n",
    "    \n",
    "    #Get all of the words in sentences(content) and phrases(subjects)\n",
    "    word_list_content = ' '.join(sentences).split(' ')   \n",
    "    word_list_subj = ' '.join(phrases).split(' ')   \n",
    "    word_list = word_list_content + word_list_subj\n",
    "    word_list=set(word_list)\n",
    "    #Number of unique words in all above\n",
    "    text_len=len(word_list)\n",
    "    \n",
    "    #Initial different sequences for list of contents and list of subjects\n",
    "    data_seq=[]\n",
    "    data_seq_subj=[]\n",
    "    \n",
    "    word_index=dictionary(word_list)\n",
    "    #Add EOS and SOS to dictionary (only for decoder,i.e.,subjects not encoder)\n",
    "    word_index[\"<SOS>\"]=text_len+1\n",
    "    word_index[\"<EOS>\"]=text_len+2\n",
    "    word_index[\"<PAD>\"]=text_len+3\n",
    "    for s in sentences:\n",
    "            s=s.split(\" \")\n",
    "            s=[word_index[w] for w in s]\n",
    "            data_seq.append(s)\n",
    "    for p in phrases:\n",
    "            p=p.split(\" \")\n",
    "            #Add SOS and EOS to subjects because we only need it for decoder \n",
    "            p.insert(0,\"<SOS>\")\n",
    "            p.insert(len(p)+1,\"<EOS>\")\n",
    "            p=[word_index[w] for w in p]\n",
    "            data_seq_subj.append(p)\n",
    "    \n",
    "    #Choose the maximum number of tokens in all sequences \n",
    "    num_tokens = [len(tokens) for tokens in data_seq]\n",
    "    max_seq_length=np.max(num_tokens)\n",
    "    \n",
    "    num_tokens_subj = [len(tokens) for tokens in data_seq_subj]\n",
    "    max_seq_subj_length=np.max(num_tokens_subj)\n",
    "    \n",
    "    #Padding sequences (using Keras)\n",
    "    #Make sequences to have the same lengths (add extra zeros to the end of the sentences)\n",
    "    #PAD's value is word_index[\"<PAD>\"]=text_len+3 \n",
    "    data_seq = pad_sequences(data_seq, maxlen = max_seq_length,\n",
    "                                padding='post', truncating='pre', value=word_index[\"<PAD>\"])\n",
    "    data_seq_subj = pad_sequences(data_seq_subj, maxlen = max_seq_subj_length,\n",
    "                                padding='post', truncating='pre', value=word_index[\"<PAD>\"])\n",
    "    \n",
    "    return word_index,data_seq,data_seq_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dictionary from list of words in text\n",
    "def dictionary(words):\n",
    "    #Create list of words without their duplications \n",
    "    words=set(words)\n",
    "    #Map word to index\n",
    "    indx = {key: i for i, key in enumerate(words)}\n",
    "    return indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from index to word\n",
    "def get_by_key_dict(indx_word,words_dict):\n",
    "    for word, indx in words_dict.iteritems():    \n",
    "        if indx == indx_word:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words to numbers\n",
    "word_index,data_sequences,data_seq_subj=encode_words_in_seqences(subject_messages,emails_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mdbe': 0,\n",
       " '': 1,\n",
       " 'gai': 121841,\n",
       " 'Craziness': 3,\n",
       " 'EXPLAIN': 4,\n",
       " 'chudson': 5,\n",
       " 'Pront': 6,\n",
       " 'woods': 7,\n",
       " 'Derike': 8,\n",
       " 'spiders': 9,\n",
       " 'Connerty': 10,\n",
       " 'woody': 11,\n",
       " 'Prone': 12,\n",
       " 'ministration': 160619,\n",
       " 'O8LG9H': 13,\n",
       " 'Eury': 14,\n",
       " 'Y1A': 15,\n",
       " 'gab': 16,\n",
       " 'Archuleta': 17,\n",
       " 'Journey': 152975,\n",
       " 'Boedecker': 19,\n",
       " 'patying': 20,\n",
       " 'Retreat': 21,\n",
       " 'Euro': 22,\n",
       " 'Tootie': 23,\n",
       " 'CORRIDOR': 24,\n",
       " 'Brianmmorgan': 25,\n",
       " 'Valle': 26,\n",
       " 'uietly': 27,\n",
       " 'Mizell': 28,\n",
       " 'WATERMELON': 29,\n",
       " 'gencos': 30,\n",
       " 'Morten': 31,\n",
       " 'Statdat': 189,\n",
       " 'bringing': 33,\n",
       " 'wooded': 34,\n",
       " 'Reconcile': 223,\n",
       " 'wooden': 36,\n",
       " 'Miers': 37,\n",
       " 'wednesday': 38,\n",
       " 'Sack': 39,\n",
       " 'virtuosos': 40,\n",
       " 'tepenovitch': 67529,\n",
       " 'Bonanz2002alpha': 41,\n",
       " 'Sacr': 42,\n",
       " 'ToolGraphs': 43,\n",
       " 'Pinkerton': 44,\n",
       " 'mv94014': 45,\n",
       " 'gaskets': 46,\n",
       " 'convo': 47,\n",
       " 'Shocked': 48,\n",
       " 'Hughlette': 50,\n",
       " 'consenting': 51,\n",
       " 'Honorable': 52,\n",
       " 'stohn': 53,\n",
       " 'snuggled': 54,\n",
       " 'USAFA': 55,\n",
       " 'nalysis': 56,\n",
       " 'dialogs': 57,\n",
       " 'warmongering': 58,\n",
       " 'usenet': 60,\n",
       " 'College': 61,\n",
       " 'Glassford': 62,\n",
       " 'Mucky': 63,\n",
       " 'Happening': 64,\n",
       " 'VNTR': 65,\n",
       " 'IPW073001': 66,\n",
       " 'cpqcorp': 67,\n",
       " 'mailings': 68,\n",
       " 'Liquefaction': 69,\n",
       " '27m': 70,\n",
       " 'affiliates': 71,\n",
       " 'quagmires': 72,\n",
       " 'Nebergall': 73,\n",
       " 'Pozdowienia': 74,\n",
       " 'Transac': 75,\n",
       " 'GlycerLEAN': 76,\n",
       " '27x': 77,\n",
       " 'affiliated': 78,\n",
       " 'Footnotes': 79,\n",
       " 'Ferlin': 80,\n",
       " 'Manger': 81,\n",
       " '27A': 82,\n",
       " '27B': 83,\n",
       " 'kids': 84,\n",
       " 'dealdine': 85,\n",
       " 'uplifting': 86,\n",
       " 'MATCHES': 87,\n",
       " 'TRAY': 88,\n",
       " 'deferring': 89,\n",
       " 'Feathers': 90,\n",
       " '27M': 91,\n",
       " 'controversy': 93,\n",
       " 'mbuchberg': 94,\n",
       " 'kidd': 95,\n",
       " 'Keillor': 96,\n",
       " 'neurologist': 97,\n",
       " 'kidk': 98,\n",
       " 'LAUGHTER': 99,\n",
       " 'authenication': 100,\n",
       " 'Acenaphthylene': 101,\n",
       " 'excuse': 165367,\n",
       " 'cranium': 103,\n",
       " '09Tyco': 104,\n",
       " 'Aker': 105,\n",
       " 'Relator': 106,\n",
       " 'overstocks': 121866,\n",
       " 'dna': 108,\n",
       " 'dnb': 109,\n",
       " 'dnm': 110,\n",
       " 'dni': 111,\n",
       " 'Leilani': 112,\n",
       " 'Crawfo': 113,\n",
       " 'CASIO': 114,\n",
       " 'PECAN': 115,\n",
       " 'kmoody': 116,\n",
       " 'dns': 117,\n",
       " 'dnr': 118,\n",
       " 'DA18': 120,\n",
       " 'MyBeautyCenter': 121,\n",
       " 'rholcomb': 122,\n",
       " 'thermoking': 123,\n",
       " 'populations': 124,\n",
       " 'ZIP': 176007,\n",
       " 'yahoo': 125,\n",
       " 'meteorologist': 126,\n",
       " 'Gaspart': 127,\n",
       " 'RECURRING': 128,\n",
       " 'gencies': 129,\n",
       " 'blustering': 130,\n",
       " 'listenin': 131,\n",
       " 'Indigo': 132,\n",
       " '09Okay': 133,\n",
       " 'TROJ_PWS': 134,\n",
       " 'Successor': 135,\n",
       " 'expressively': 136,\n",
       " 'Hiresynergy': 137,\n",
       " 'T13S': 138,\n",
       " 'quickinspiration': 811,\n",
       " 'xRuth': 140,\n",
       " 'disparagement': 141,\n",
       " 'titanium': 142,\n",
       " 'LatinMail': 143,\n",
       " 'Suckered': 840,\n",
       " 'pinto': 145,\n",
       " 'Oberoi': 146,\n",
       " 'Receiving': 147,\n",
       " 'SCORERS': 148,\n",
       " 'beyer': 149,\n",
       " 'KINGERSKI': 150,\n",
       " 'SURROUNDED': 151,\n",
       " 'Nussbaum': 152,\n",
       " 'portesters': 153,\n",
       " 'PEPCO': 155,\n",
       " 'xonClick': 156,\n",
       " 'Correspondent': 157,\n",
       " 'RESERVED': 158,\n",
       " 'goober': 159,\n",
       " 'TERM': 30276,\n",
       " 'dinosaurs': 161,\n",
       " 'wrong': 162,\n",
       " 'Fogg': 163,\n",
       " 'sentencing': 164,\n",
       " 'Fogo': 165,\n",
       " 'Tenants': 166,\n",
       " 'Unspent': 167,\n",
       " 'RAA52811': 168,\n",
       " 'Stickman': 169,\n",
       " 'TERI': 30279,\n",
       " 'LIEBMAN': 171,\n",
       " 'CEPSA': 172,\n",
       " 'technologicp': 173,\n",
       " 'Debaters': 1042,\n",
       " 'Anjali': 175,\n",
       " 'LAGOON': 176,\n",
       " 'DFARMER': 177,\n",
       " 'CARTOON': 178,\n",
       " 'Retention': 179,\n",
       " 'Bakeware': 180,\n",
       " 'ORBITAL': 60571,\n",
       " 'snugly': 182,\n",
       " 'welcomed': 183,\n",
       " 'Stayover': 184,\n",
       " 'concurrence': 185,\n",
       " 'Rated': 187,\n",
       " 'activating': 188,\n",
       " 'PORTLAND': 32,\n",
       " 'welcomes': 190,\n",
       " 'fir': 191,\n",
       " 'Evolve': 192,\n",
       " 'His': 193,\n",
       " 'Hit': 194,\n",
       " '3DNmartin': 195,\n",
       " 'fit': 196,\n",
       " 'fiu': 197,\n",
       " 'HYLA': 45466,\n",
       " 'screaming': 199,\n",
       " 'fix': 200,\n",
       " 'Rubino': 201,\n",
       " 'fic': 203,\n",
       " 'fia': 204,\n",
       " 'fig': 205,\n",
       " 'Hig': 206,\n",
       " 'fik': 207,\n",
       " 'fin': 208,\n",
       " 'Him': 1208,\n",
       " 'fil': 210,\n",
       " 'fim': 211,\n",
       " 'pkozlowski': 60575,\n",
       " 'pgg4': 30289,\n",
       " 'Aloe': 214,\n",
       " 'Alok': 215,\n",
       " 'REVERSED': 216,\n",
       " 'Alon': 217,\n",
       " 'vouchers': 218,\n",
       " 'inactivations': 219,\n",
       " 'Alot': 220,\n",
       " 'LOSING': 221,\n",
       " 'graduare': 222,\n",
       " 'grueling': 35,\n",
       " 'lucere': 224,\n",
       " 'Hi8': 225,\n",
       " 'castigating': 226,\n",
       " 'DECISION': 227,\n",
       " 'barton': 228,\n",
       " '57PM': 229,\n",
       " 'LAA16142': 230,\n",
       " 'Combine': 231,\n",
       " 'l96106': 232,\n",
       " 'Elbertson': 233,\n",
       " 'TERESA': 234,\n",
       " 'Sach': 235,\n",
       " 'Cymrot': 237,\n",
       " 'volumesto': 238,\n",
       " 'nieces': 13509,\n",
       " '5_02': 239,\n",
       " 'TF12': 241,\n",
       " 'Basement': 242,\n",
       " 'size': 243,\n",
       " 'buenos': 21515,\n",
       " 'Tastefully': 245,\n",
       " 'abbott': 246,\n",
       " 'AutoBodyOnline': 247,\n",
       " 'PAYS': 248,\n",
       " 'BUCHANAN': 249,\n",
       " 'syn': 121892,\n",
       " 'instructio': 250,\n",
       " 'Corinne': 251,\n",
       " 'pumpkins': 252,\n",
       " 'Corinna': 253,\n",
       " 'Officializations': 254,\n",
       " 'Tillett': 255,\n",
       " 'PTOs': 256,\n",
       " 'PAYE': 257,\n",
       " 'sys': 121894,\n",
       " 'Mexicoto': 259,\n",
       " 'Nigel': 260,\n",
       " 'danelrr': 261,\n",
       " 'Reservat': 262,\n",
       " 'NCO1': 263,\n",
       " 'Localization': 158477,\n",
       " 'absolved': 265,\n",
       " 'SIGNED': 266,\n",
       " 'Reservas': 267,\n",
       " 'perators': 268,\n",
       " 'SIGNER': 269,\n",
       " 'wwpco': 270,\n",
       " 'grandbankscasino': 271,\n",
       " 'yester': 272,\n",
       " 'MUS17613': 273,\n",
       " 'Ibrahims': 274,\n",
       " 'harvardclubofhouston': 275,\n",
       " 'LANs': 276,\n",
       " 'megabytes': 277,\n",
       " 'Clute': 14075,\n",
       " 'LANG': 278,\n",
       " 'olds': 279,\n",
       " 'LAND': 280,\n",
       " 'renovated': 281,\n",
       " 'tradersnewsenergy': 282,\n",
       " 'Avaya': 283,\n",
       " 'LANI': 284,\n",
       " 'needed': 285,\n",
       " 'LANS': 286,\n",
       " 'fuzziness': 287,\n",
       " 'genesis': 288,\n",
       " 'GOIP': 289,\n",
       " 'Hammerstein': 290,\n",
       " 'Biologo': 155360,\n",
       " 'Dolvi': 291,\n",
       " 'complainers': 91172,\n",
       " 'Tripwire': 30308,\n",
       " 'tidepool': 294,\n",
       " 'Javens': 295,\n",
       " 'Skalmeraas': 296,\n",
       " 'tionships': 297,\n",
       " 'TISWG': 298,\n",
       " 'Rania': 299,\n",
       " 'positively': 300,\n",
       " 'Sookeun': 301,\n",
       " 'exclaimed': 302,\n",
       " 'Shrinkage': 74926,\n",
       " 'SPEED': 303,\n",
       " 'similaires': 304,\n",
       " 'homunculus_2001': 305,\n",
       " 'ecohan': 306,\n",
       " 'Chicago': 307,\n",
       " 'reurn': 308,\n",
       " 'Recruiting': 309,\n",
       " 'MJBCTFUREY': 310,\n",
       " 'recapitalized': 311,\n",
       " 'Theo': 312,\n",
       " 'Then': 313,\n",
       " 'Them': 314,\n",
       " 'credentialed': 315,\n",
       " 'albertapower': 316,\n",
       " 'Theg': 318,\n",
       " 'Thee': 319,\n",
       " 'Thea': 320,\n",
       " 'They': 321,\n",
       " 'Lakshmi': 322,\n",
       " 'Fusco': 323,\n",
       " 'Neeloofar': 324,\n",
       " 'greenback': 2051,\n",
       " 'shipments': 326,\n",
       " '45th': 327,\n",
       " 'determinination': 328,\n",
       " 'documentatio': 329,\n",
       " 'DVE631': 330,\n",
       " 'reverence': 331,\n",
       " 'hardwoods': 30314,\n",
       " 'Centralize': 333,\n",
       " 'majo': 334,\n",
       " 'tect': 335,\n",
       " 'theIFM': 336,\n",
       " '00_00012': 337,\n",
       " '00_00010': 338,\n",
       " 'teco': 339,\n",
       " 'tech': 340,\n",
       " 'Karris': 341,\n",
       " 'EarthSat': 342,\n",
       " '09Pipes': 343,\n",
       " 'braising': 344,\n",
       " 'tempter': 345,\n",
       " 'tempted': 346,\n",
       " 'Basinger': 347,\n",
       " 'hounded': 348,\n",
       " 'Weijing': 350,\n",
       " 'apace': 351,\n",
       " 'Barefoot': 352,\n",
       " 'lubr': 353,\n",
       " 'orola': 354,\n",
       " 'shoppping': 355,\n",
       " 'luby': 356,\n",
       " 'lube': 357,\n",
       " '2YEAR24': 358,\n",
       " 'cnamprem': 359,\n",
       " 'houstonchronicle': 360,\n",
       " 'Energize': 361,\n",
       " 'COOKING': 362,\n",
       " 'iate': 363,\n",
       " 'rick_noger': 364,\n",
       " 'afala': 365,\n",
       " 'eAnglerWalleye': 366,\n",
       " 'Ecology': 367,\n",
       " 'onight': 2385,\n",
       " 'Fascinating': 369,\n",
       " 'Andover': 370,\n",
       " 'patch': 371,\n",
       " 'Dances': 372,\n",
       " 'Dancer': 373,\n",
       " 'Kerudi': 374,\n",
       " 'Kollaros': 60604,\n",
       " 'K9DGDD9X': 376,\n",
       " 'programmatic': 377,\n",
       " 'CSUP': 85037,\n",
       " 'heirloom': 378,\n",
       " 'mlurie': 379,\n",
       " 'Landivar': 380,\n",
       " 'Anderson': 381,\n",
       " '2T2BRED': 382,\n",
       " 'CRESTING': 383,\n",
       " 'ahorrar': 384,\n",
       " 'iry': 385,\n",
       " 'Cotchett': 386,\n",
       " 'irq': 387,\n",
       " 'irr': 388,\n",
       " 'irs': 389,\n",
       " 'Trinitron': 390,\n",
       " 'motional': 391,\n",
       " 'irk': 392,\n",
       " 'timesheets': 393,\n",
       " 'iro': 394,\n",
       " 'conductive': 395,\n",
       " 'ira': 396,\n",
       " 'ire': 397,\n",
       " 'irg': 398,\n",
       " 'Doctorates': 399,\n",
       " 'discipline': 400,\n",
       " 'ClickHereForCellPhones': 401,\n",
       " 'natura': 402,\n",
       " 'extend': 403,\n",
       " 'nature': 404,\n",
       " 'SB1712': 405,\n",
       " 'fruits': 406,\n",
       " 'vestment': 407,\n",
       " 'ndrgroup': 408,\n",
       " 'sletter': 409,\n",
       " 'extent': 410,\n",
       " 'rdw0': 411,\n",
       " 'Leffingwell': 412,\n",
       " 'Sanjo': 413,\n",
       " 'Leppez': 414,\n",
       " 'StkScape_unsub': 415,\n",
       " 'PREQUALIFICAT': 60611,\n",
       " '27s': 417,\n",
       " 'FACTOR': 418,\n",
       " 'Coorperative': 419,\n",
       " 'Pebble': 420,\n",
       " 'ASHSI': 421,\n",
       " 'Balke': 422,\n",
       " 'foreclosing': 423,\n",
       " 'Gymboree': 424,\n",
       " '09Phone': 425,\n",
       " 'Balks': 426,\n",
       " 'humming': 427,\n",
       " 'Instantly': 428,\n",
       " 'Montagne': 429,\n",
       " 'underdone': 430,\n",
       " 'VoiceMail': 431,\n",
       " 'gnichols': 432,\n",
       " 'Cucumber': 433,\n",
       " 'Haslett': 434,\n",
       " 'obese': 435,\n",
       " 'regul': 436,\n",
       " 'Manges': 437,\n",
       " 'EPMICALPOOL': 438,\n",
       " 'Bereuter': 439,\n",
       " 'Chads': 440,\n",
       " 'jmangold': 121926,\n",
       " '1670N': 442,\n",
       " 'ESVL': 153052,\n",
       " 'doubts': 444,\n",
       " 'Clas': 78966,\n",
       " 'regus': 445,\n",
       " 'tgadsden': 446,\n",
       " 'propellants': 447,\n",
       " '27E': 448,\n",
       " 'mdobbs': 449,\n",
       " 'LONNA': 450,\n",
       " 'professionally': 451,\n",
       " 'DeVivieros': 452,\n",
       " 'misconstrued': 453,\n",
       " 'BW0107': 454,\n",
       " 'Combined': 153053,\n",
       " 'Vanderjagt': 455,\n",
       " 'canoeing': 456,\n",
       " 'desti': 2954,\n",
       " 'Echterhoff': 458,\n",
       " 'Monosp': 459,\n",
       " 'Austinites': 460,\n",
       " 'ECTRS': 180944,\n",
       " 'EB1': 461,\n",
       " 'waddell': 462,\n",
       " 'dg7023_14013': 463,\n",
       " 'Embossed': 464,\n",
       " '3Dfx': 465,\n",
       " 'Tankersley': 466,\n",
       " 'memorial': 467,\n",
       " 'remand': 468,\n",
       " 'equistar': 469,\n",
       " 'C420': 470,\n",
       " '09Chemicals': 471,\n",
       " 'rehypothecated': 121930,\n",
       " '09Cavallo': 3052,\n",
       " '505k': 474,\n",
       " 'Maybe': 475,\n",
       " 'NATION': 476,\n",
       " 'PJs': 78967,\n",
       " '27X': 477,\n",
       " '0F_47': 478,\n",
       " 'weffective': 479,\n",
       " 'Tahari': 480,\n",
       " 'EBD': 482,\n",
       " 'EBE': 483,\n",
       " 'EBF': 484,\n",
       " 'furlough': 485,\n",
       " 'EBA': 486,\n",
       " 'EBB': 3121,\n",
       " 'EBC': 488,\n",
       " 'EBM': 489,\n",
       " 'EBN': 490,\n",
       " 'EBI': 491,\n",
       " 'darkside': 492,\n",
       " 'luike': 3149,\n",
       " 'EBT': 494,\n",
       " 'Supermodel': 3155,\n",
       " 'EBP': 496,\n",
       " 'EBR': 497,\n",
       " 'EBS': 498,\n",
       " 'mentorg': 499,\n",
       " 'corporate': 500,\n",
       " 'massaging': 501,\n",
       " 'lletto': 502,\n",
       " 'explanantion': 503,\n",
       " 'bellow': 504,\n",
       " 'stny': 505,\n",
       " 'Laureate': 506,\n",
       " 'Deacero': 3255,\n",
       " 'visability': 508,\n",
       " 'HBROWN': 509,\n",
       " 'immeadiately': 510,\n",
       " 'McJames': 511,\n",
       " 'upsides': 512,\n",
       " 'knipe3': 513,\n",
       " 'SPECIALTY': 3313,\n",
       " 'SPECIFIC': 515,\n",
       " 'mcnamarg': 3332,\n",
       " 'SPIEGEL': 517,\n",
       " 'duffel': 518,\n",
       " 'Larkoworthy': 519,\n",
       " 'opportuntity': 60632,\n",
       " 'UTStarcom': 3367,\n",
       " 'inging': 522,\n",
       " 'Setup': 523,\n",
       " 'REQUEST': 524,\n",
       " 'unequivocally': 525,\n",
       " 'indicative': 526,\n",
       " 'BAVRA': 527,\n",
       " 'ROMANT': 528,\n",
       " 'Quickie': 529,\n",
       " 'x3103': 183594,\n",
       " 'sleuthing': 530,\n",
       " 'EB4922': 531,\n",
       " 'PRESSES': 532,\n",
       " 'bambos': 533,\n",
       " 'Brenda': 534,\n",
       " '167k': 535,\n",
       " 'Liebermann': 536,\n",
       " 'ctims': 537,\n",
       " 'WEXFORD': 538,\n",
       " 'expatiari': 539,\n",
       " 'Creacion': 3464,\n",
       " 'utilites': 541,\n",
       " 'Frames': 542,\n",
       " 'Pentagon': 544,\n",
       " '57B054': 545,\n",
       " 'Swedish': 546,\n",
       " 'jherring': 547,\n",
       " 'Nearl': 121943,\n",
       " 'Estac': 549,\n",
       " 'crowd': 550,\n",
       " 'Homestretch': 91221,\n",
       " 'crown': 552,\n",
       " 'crows': 553,\n",
       " 'Estat': 554,\n",
       " 'Overpull': 555,\n",
       " 'Estar': 556,\n",
       " '09Super': 557,\n",
       " 'emphases': 558,\n",
       " 'fiduciary': 559,\n",
       " 'cmarinucci': 560,\n",
       " 'Demyanovich': 561,\n",
       " 'DVIN': 562,\n",
       " 'Couples': 563,\n",
       " 'perchance': 564,\n",
       " 'PLORE': 565,\n",
       " 'completly': 566,\n",
       " 'Celebration': 567,\n",
       " 'doctorbonner': 568,\n",
       " 'Saji': 570,\n",
       " 'Wolfgan': 571,\n",
       " 'Calls': 572,\n",
       " 'Accompanying': 573,\n",
       " 'Nguy': 574,\n",
       " 'Calle': 575,\n",
       " 'Kentuck': 576,\n",
       " 'Calll': 577,\n",
       " 'exersized': 578,\n",
       " 'Simitian': 579,\n",
       " 'malmuth': 580,\n",
       " 'restlessness': 581,\n",
       " 'spaceships': 582,\n",
       " 'anomalous': 583,\n",
       " 'CUSTOMS': 584,\n",
       " 'officeholders': 585,\n",
       " 'kilgore': 586,\n",
       " 'temorarily': 587,\n",
       " 'xpanded': 588,\n",
       " 'M326403': 589,\n",
       " 'marshall': 591,\n",
       " 'honeymoon': 592,\n",
       " 'nongovernmental': 593,\n",
       " 'Sitka': 594,\n",
       " 'bhansen': 595,\n",
       " 'marshals': 596,\n",
       " 'jason_j_marsh': 597,\n",
       " 'Unrestricted': 598,\n",
       " 'sidebars': 599,\n",
       " 'Troughton': 600,\n",
       " 'Ends': 601,\n",
       " 'despised': 602,\n",
       " 'fabric': 603,\n",
       " 'Vernon': 604,\n",
       " 'raped': 605,\n",
       " 'Pubs': 3863,\n",
       " 'LRSolutions': 607,\n",
       " 'grasping': 608,\n",
       " 'despises': 609,\n",
       " 'rapes': 610,\n",
       " '12OCT': 611,\n",
       " 'BRESSERT': 91231,\n",
       " 'lanefinancialllc': 613,\n",
       " 'Affymetrix': 614,\n",
       " 'PhotoAlley': 615,\n",
       " 'CONTROL_FIELD_SIZE': 616,\n",
       " 'BTGC': 617,\n",
       " 'Roast': 618,\n",
       " 'congratulations': 620,\n",
       " 'humbled': 621,\n",
       " 'Household': 623,\n",
       " 'Groveland': 624,\n",
       " 'clause': 625,\n",
       " 'everyboby': 626,\n",
       " 'Ruppe': 627,\n",
       " 'humbles': 628,\n",
       " 'nicest': 629,\n",
       " 'spaying': 630,\n",
       " 'FTAA01': 631,\n",
       " 'pencak': 632,\n",
       " '55xx': 633,\n",
       " 'passenger': 634,\n",
       " 'creuset': 635,\n",
       " 'YOUNG': 636,\n",
       " 'Poinsettia': 637,\n",
       " 'Crater': 638,\n",
       " 'uppercased': 60650,\n",
       " 'ANAGEMENT': 640,\n",
       " 'bellatlantic': 641,\n",
       " 'w746937': 642,\n",
       " 'FitzPatrick': 643,\n",
       " 'crowns': 644,\n",
       " 'Cedulas': 645,\n",
       " 'INDICATING': 646,\n",
       " 'Demagogues': 647,\n",
       " 'serverless': 648,\n",
       " 'SERV': 30367,\n",
       " 'palms': 650,\n",
       " 'Aboretum': 651,\n",
       " 'Ids': 652,\n",
       " 'Stento': 653,\n",
       " 'Brunilda': 30368,\n",
       " 'Ide': 655,\n",
       " 'Ida': 657,\n",
       " 'continentally': 658,\n",
       " 'hypoallergenic': 659,\n",
       " 'overcollections': 661,\n",
       " 'Sedona0015': 91241,\n",
       " 'Unreserved': 663,\n",
       " 'steelhead': 664,\n",
       " 'Pantheo': 61295,\n",
       " 'WebOasis': 665,\n",
       " 'chaim': 666,\n",
       " 'chain': 667,\n",
       " 'whoever': 668,\n",
       " 'Quantify': 669,\n",
       " 'preapproval': 670,\n",
       " 'Spector': 60658,\n",
       " 'EnerTouch': 672,\n",
       " 'Artisan': 673,\n",
       " '609052384AISLE': 674,\n",
       " 'encourageing': 675,\n",
       " 'chair': 676,\n",
       " 'Bobbish': 677,\n",
       " 'STK14C88N': 678,\n",
       " 'ballet': 679,\n",
       " 'amplification': 680,\n",
       " 'grapples': 681,\n",
       " '75MB': 682,\n",
       " 'INVOLVED': 683,\n",
       " 'Bartosh': 684,\n",
       " 'megawatts': 685,\n",
       " 'okesperson': 686,\n",
       " 'przenioslem': 687,\n",
       " 'repathing': 688,\n",
       " 'SiteList': 689,\n",
       " 'macho': 690,\n",
       " 'Ofelia': 691,\n",
       " 'abundent': 692,\n",
       " 'SPLUS': 140385,\n",
       " 'flatbed': 693,\n",
       " 'jerk': 694,\n",
       " 'TONNAGE': 30376,\n",
       " 'EWooglin': 91248,\n",
       " 'mpfarmer': 697,\n",
       " 'lark': 698,\n",
       " 'gloomy': 699,\n",
       " 'Ruddy': 700,\n",
       " 'unpleasent': 701,\n",
       " 'Barrel': 702,\n",
       " 'husteros': 703,\n",
       " 'exact': 704,\n",
       " 'minute': 705,\n",
       " 'pic27593': 706,\n",
       " 'Hellman': 707,\n",
       " 'Knezevich': 30378,\n",
       " 'violatio': 709,\n",
       " 'illustrators': 710,\n",
       " 'Katrina': 711,\n",
       " 'Sahar': 712,\n",
       " 'Dealbench': 713,\n",
       " 'pasztor': 714,\n",
       " 'Mitts': 715,\n",
       " '3DADT': 716,\n",
       " 'Silvio': 717,\n",
       " '2001and': 718,\n",
       " 'Europes': 719,\n",
       " 'MMurray538': 720,\n",
       " 'Silvia': 721,\n",
       " 'sustantive': 722,\n",
       " 'M5': 723,\n",
       " 'M4': 724,\n",
       " 'RSCR': 725,\n",
       " 'M6': 726,\n",
       " 'M1': 4572,\n",
       " 'M0': 728,\n",
       " 'M3': 729,\n",
       " 'M2': 730,\n",
       " 'ogled': 731,\n",
       " 'ufl': 121980,\n",
       " 'Mazotti': 733,\n",
       " 'randolph02': 734,\n",
       " 'M8': 735,\n",
       " 'ranteed': 736,\n",
       " 'Substitutes': 737,\n",
       " 'norther': 121981,\n",
       " 'Bassal': 739,\n",
       " 'Keeton': 740,\n",
       " 'Phun': 60672,\n",
       " 'Travella': 742,\n",
       " 'Premise': 743,\n",
       " 'McKi': 744,\n",
       " 'geography': 745,\n",
       " 'unintentionally': 746,\n",
       " 'Me': 747,\n",
       " 'Affeldt': 4652,\n",
       " 'Ma': 749,\n",
       " 'jitin': 750,\n",
       " 'oldies': 4664,\n",
       " 'Mb': 752,\n",
       " 'Mm': 753,\n",
       " 'Ml': 754,\n",
       " 'Mo': 755,\n",
       " 'CONTRIBUTE': 756,\n",
       " 'Mi': 757,\n",
       " 'Mk': 758,\n",
       " 'Uniphase': 759,\n",
       " 'TWAIN': 760,\n",
       " 'Mt': 761,\n",
       " 'Mw': 762,\n",
       " 'Infrastructure': 763,\n",
       " 'Taskforce': 764,\n",
       " 'Bao_Ha': 765,\n",
       " 'Mr': 766,\n",
       " 'Tubing': 767,\n",
       " 'ponding': 4710,\n",
       " 'benson': 769,\n",
       " 'ME': 770,\n",
       " 'MD': 771,\n",
       " 'MG': 772,\n",
       " 'MF': 773,\n",
       " 'MA': 774,\n",
       " 'MC': 775,\n",
       " 'MB': 776,\n",
       " 'MM': 777,\n",
       " 'ML': 778,\n",
       " 'MO': 779,\n",
       " 'MN': 780,\n",
       " 'MI': 781,\n",
       " 'MH': 782,\n",
       " 'MK': 783,\n",
       " 'MJ': 784,\n",
       " 'MU': 785,\n",
       " 'RDEL': 4757,\n",
       " 'MW': 787,\n",
       " 'RDEN': 788,\n",
       " 'MQ': 789,\n",
       " 'MP': 790,\n",
       " 'MS': 791,\n",
       " 'MR': 792,\n",
       " 'Flack': 793,\n",
       " '354hrs': 794,\n",
       " 'MY': 795,\n",
       " 'MX': 796,\n",
       " 'Jardin': 797,\n",
       " 'DRN_description': 798,\n",
       " 'gidday': 799,\n",
       " 'PRAYR786': 800,\n",
       " 'EXCEL': 801,\n",
       " 'Blancke': 802,\n",
       " 'S1050': 803,\n",
       " 'Jinsung': 804,\n",
       " 'Midrand': 805,\n",
       " 'ABAG': 91263,\n",
       " 'POWX': 807,\n",
       " 'August': 808,\n",
       " 'handbags': 809,\n",
       " 'perished': 810,\n",
       " 'pints': 139,\n",
       " 'Wristwatch': 812,\n",
       " 'ARNOLD': 813,\n",
       " 'Averting': 814,\n",
       " 'epmi_': 815,\n",
       " 'POWI': 816,\n",
       " 'Tolerates': 817,\n",
       " 'liberalizing': 818,\n",
       " 'Those': 820,\n",
       " 'Flumiani': 821,\n",
       " 'ADVANCES': 4937,\n",
       " 'Implications': 823,\n",
       " 'approving': 824,\n",
       " 'CONTENT': 825,\n",
       " '09So': 826,\n",
       " 'Devali': 827,\n",
       " 'bharati': 828,\n",
       " 'STANISLAUS': 829,\n",
       " '09Sa': 830,\n",
       " 'ADVANCED': 831,\n",
       " 'rebounding': 832,\n",
       " 'EXPORTS': 833,\n",
       " 'JJW': 834,\n",
       " 'Cabling': 835,\n",
       " 'rutherford': 836,\n",
       " 'Hershey': 837,\n",
       " 'RECEIVERSHIP': 838,\n",
       " 'following': 839,\n",
       " 'iCurrent': 144,\n",
       " 'JJC': 841,\n",
       " 'Whois': 842,\n",
       " 'zled': 843,\n",
       " 'Symphony': 844,\n",
       " 'Pichel': 845,\n",
       " 'Bijan': 846,\n",
       " 'Ahman': 153107,\n",
       " 'TripAlert': 848,\n",
       " 'Shiga': 849,\n",
       " 'litre': 850,\n",
       " 'Oberon': 851,\n",
       " 'spaper': 852,\n",
       " 'Agnihotri': 853,\n",
       " 'ESTIMATES': 5142,\n",
       " 'thanking': 855,\n",
       " 'djhyvl': 856,\n",
       " 'Glitters': 165310,\n",
       " 'Significant': 857,\n",
       " 'mymailstation': 858,\n",
       " 'ESTIMATED': 859,\n",
       " 'consolidator': 860,\n",
       " 'rgot': 861,\n",
       " 'RIzvi': 862,\n",
       " 'taj1068': 863,\n",
       " 'marketwatch': 133002,\n",
       " 'thereaboot': 864,\n",
       " 'Resaving': 866,\n",
       " 'Hermitte': 867,\n",
       " 'fueled': 868,\n",
       " 'COOLING': 124126,\n",
       " 'Bert_Leonard': 869,\n",
       " 'cacciotti': 870,\n",
       " 'Editorials': 871,\n",
       " 'ZETA': 872,\n",
       " '093876B': 873,\n",
       " '093876C': 874,\n",
       " 'Interchanges': 875,\n",
       " 'surfing': 876,\n",
       " 'DataBase': 877,\n",
       " 'Schefter': 878,\n",
       " '_Get_4_DVDs_for_49': 5294,\n",
       " 'hartfam6': 91277,\n",
       " 'skirmish': 881,\n",
       " 'impactful': 882,\n",
       " 'LIE': 883,\n",
       " 'LIF': 884,\n",
       " 'LIG': 885,\n",
       " 'ATTEMPT': 886,\n",
       " 'LIA': 887,\n",
       " 'Bowie': 888,\n",
       " 'LIL': 889,\n",
       " 'LIM': 890,\n",
       " 'LIN': 891,\n",
       " 'LIH': 892,\n",
       " 'x31653': 5362,\n",
       " 'LIU': 894,\n",
       " 'LIP': 895,\n",
       " 'Mehendale': 896,\n",
       " 'LIZ': 897,\n",
       " 'GT12': 898,\n",
       " 'jig': 899,\n",
       " 'GT11': 900,\n",
       " 'jib': 901,\n",
       " 'OFFER': 902,\n",
       " 'jin': 903,\n",
       " 'jim': 904,\n",
       " 'Scot_Lawrence': 905,\n",
       " 'Agencies': 906,\n",
       " 'cooperations': 122004,\n",
       " 'eAngler': 908,\n",
       " 'fburmeister': 910,\n",
       " 'bc1000': 911,\n",
       " 'LIHEAP': 912,\n",
       " 'Hiding': 913,\n",
       " 'apron': 914,\n",
       " 'EB1750': 915,\n",
       " 'Iraqis': 916,\n",
       " 'esnure': 917,\n",
       " 'MURPHY': 918,\n",
       " 'bashful': 919,\n",
       " 'Mameluke': 920,\n",
       " 'EB1756': 921,\n",
       " 'CHOICES': 922,\n",
       " 'overpowering': 924,\n",
       " 'Aboard': 925,\n",
       " 'EDITORIAL': 926,\n",
       " 'aprox': 927,\n",
       " 'sorted': 928,\n",
       " 'jan2002': 929,\n",
       " 'MATCHING': 930,\n",
       " 'PROFITABLE': 931,\n",
       " 'FROGZ': 932,\n",
       " 'FROGS': 933,\n",
       " 'Nicotrol': 934,\n",
       " 'Bernadette': 935,\n",
       " 'nckmc': 936,\n",
       " 'instability': 937,\n",
       " 'personalizing': 938,\n",
       " 'quarter': 939,\n",
       " 'quartet': 940,\n",
       " 'Blaze': 30411,\n",
       " 'FASB133': 942,\n",
       " 'honduras': 943,\n",
       " 'Yoshiaki': 944,\n",
       " 'xchanges': 945,\n",
       " '20010908B': 946,\n",
       " 'StockWatch': 947,\n",
       " 'Shearman': 30413,\n",
       " 'tianfengus': 949,\n",
       " 'Bates': 950,\n",
       " 'BOATERSWORLD': 951,\n",
       " 'salads': 952,\n",
       " 'Deborah': 953,\n",
       " 'rski': 954,\n",
       " 'effectve': 955,\n",
       " 'Michigan': 956,\n",
       " 'Immelt': 957,\n",
       " 'Rabalais': 958,\n",
       " 'UJSDNTNO': 959,\n",
       " 'T55N': 960,\n",
       " 'nearGuiriato': 961,\n",
       " 'Bouvier': 962,\n",
       " 'Yallourn': 963,\n",
       " 'Counteroffer': 964,\n",
       " 'Recital': 965,\n",
       " 'Presentations': 966,\n",
       " 'Seade': 967,\n",
       " 'inwardly': 968,\n",
       " 'touchent': 969,\n",
       " 'workmanship': 970,\n",
       " 'Venter': 971,\n",
       " 'aplethora': 30416,\n",
       " 'HEAT': 91290,\n",
       " 'x35082': 974,\n",
       " 'Pierret': 975,\n",
       " 'quibble': 976,\n",
       " 'socking': 977,\n",
       " 'Yucaipa': 978,\n",
       " 'blnk': 979,\n",
       " 'Backend': 980,\n",
       " 'Norwalk': 981,\n",
       " 'spoked': 982,\n",
       " 'spoken': 983,\n",
       " 'SLUMP': 75345,\n",
       " 'tableaux': 985,\n",
       " 'owers': 986,\n",
       " 'Fates': 987,\n",
       " 'EMUMAIL': 988,\n",
       " 'concert': 32290,\n",
       " 'MECHANICS': 990,\n",
       " 'Enron_Paper_1_27_01': 991,\n",
       " 'NRKRP': 992,\n",
       " 'HESCO': 993,\n",
       " 'Fogh': 994,\n",
       " 'Greenamber5': 995,\n",
       " 'AGENT': 996,\n",
       " 'Cincinatti': 60705,\n",
       " 'mmcfd': 998,\n",
       " 'Barbar': 999,\n",
       " 'Safeguard': 1000,\n",
       " '1cjlpw8': 1001,\n",
       " 'ocurred': 1002,\n",
       " 'Bomba': 1003,\n",
       " ...}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[184969,  36307, 148992, ..., 184971, 184971, 184971],\n",
       "       [184969,  36307, 148992, ..., 184971, 184971, 184971],\n",
       "       [184969, 165306,  38393, ..., 184971, 184971, 184971],\n",
       "       ..., \n",
       "       [184969,  43121,  71731, ..., 184971, 184971, 184971],\n",
       "       [184969,  12222,  52830, ..., 184971, 184971, 184971],\n",
       "       [184969, 119107,  14178, ..., 184971, 184971, 184971]], dtype=int32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sequences for subjects\n",
    "data_seq_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165068, 178094, 147508, ..., 184971, 184971, 184971],\n",
       "       [165068, 178094, 147508, ..., 184971, 184971, 184971],\n",
       "       [165306,  38393, 107130, ..., 184971, 184971, 184971],\n",
       "       ..., \n",
       "       [107130, 145346,  95395, ..., 184971, 184971, 184971],\n",
       "       [ 57993,  81199,  48884, ..., 184971, 184971, 184971],\n",
       "       [ 40449,   8915, 138223, ..., 184971, 184971, 184971]], dtype=int32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sequences for contents\n",
    "data_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1013"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the input and target lengths (after padding)\n",
    "input_len=len(data_sequences[0])\n",
    "target_len=len(data_seq_subj[0])\n",
    "input_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the naive approach for embedding (which is initializing the embedding vectors with random numbers and then let our model to further learn the embeddings), we can use GloVe to initialize some of the embeddings with pre_trained data learned from GloVe. In this case, the words in our text or dictionary that exist in GloVe are initialize with the word embeddings learned from GloVe and for the rest of words we may use the embedding vectors with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n",
      "[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "  0.67204 ]\n"
     ]
    }
   ],
   "source": [
    "glovefile=\"/glovedata/glove.6B.50d.txt\"\n",
    "#Load Glove model from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "def loadGloveModel(gloveFile):\n",
    "    words_in_glove=[]\n",
    "    print \"Loading Glove Model\"\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        words_in_glove.append(word)\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model,words_in_glove\n",
    "model,words_in_glove=loadGloveModel(glovefile)\n",
    "\n",
    "print model['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings of GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(vocabulary_size,embedding_size,word_index,words_in_glove):\n",
    "    #For the words in the emails data that are not in the glove we give them random embeddings vectors\n",
    "    embedding_all= tf.Variable(tf.random_uniform((vocabulary_size, embedding_size), -1, 1))\n",
    "    for i in range(0,vocabulary_size):\n",
    "        if get_by_key_dict(i,word_index) in words_in_glove:\n",
    "            tf.assign(embedding_all[i],model[get_by_key_dict(i,word_index)])\n",
    "    print(embedding_all)\n",
    "    return embedding_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  create_model_inputs():\n",
    "    '''Define model inputs'''\n",
    "    \n",
    "    #Model's placeholders for inputs\n",
    "    encoding_inputs = tf.placeholder(tf.int32, [None, None], name='encode_inputs')\n",
    "    decoder_targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='inputs_length')\n",
    "    target_length =tf.placeholder(shape=(None,), dtype=tf.int32, name='target_length')\n",
    "\n",
    "    return encoding_inputs,decoder_targets,keep_prob,inputs_length,target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    '''Using generator to return batches for train'''\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    '''In case that the batch_size is not a multiple of data size (number of sequences) in order to create batches with the same sizes, this line will ignore the last sequences that cannot create a full batch'''\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build encoder cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN_encoder_cells(num_hidden,lstm_layer_numbers,keep_prob,batch_size):\n",
    "    '''Build RNN encoder cells'''\n",
    "\n",
    "    #Define LSTM layers(Bidirectional need both backward and forward info)\n",
    "    lstms=[]\n",
    "    for i in range(lstm_layer_numbers):\n",
    "        lstms.append(tf.contrib.rnn.BasicLSTMCell(num_hidden))\n",
    "    # Add regularization dropout to the LSTM cells\n",
    "    lstm_fw_cell = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for lstm in lstms]\n",
    "    lstm_bw_cell = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for lstm in lstms]\n",
    "\n",
    "    # Stack up multiple LSTM layers\n",
    "    stacked_lstm_fw = tf.contrib.rnn.MultiRNNCell(lstm_fw_cell)\n",
    "    stacked_lstm_bw = tf.contrib.rnn.MultiRNNCell(lstm_bw_cell)\n",
    "    \n",
    "    return stacked_lstm_fw,stacked_lstm_bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build decoder cells (with attention). Attention is useful for long sentences from encoder that are needed to be paid attention by decoder to their specific words for better prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN_decoder_cells(encoder_outputs,encoder_state,num_hidden,lstm_layer_numbers,batch_size,inputs_length):\n",
    "    '''Build RNN decoder attention cells'''\n",
    "    #Define LSTM layers\n",
    "    lstms=[]\n",
    "    for i in range(lstm_layer_numbers):\n",
    "        #The concatenation of backward anf forward of LSTM cells from encoder resulted in num_hidden*2 units instead of num_hidden; thtat's what decoder should expect (num_hidden*2), same goes for attention\n",
    "        lstms.append(tf.contrib.rnn.BasicLSTMCell(num_hidden*2))\n",
    "    #Add regularization dropout to the LSTM cells\n",
    "    decoder_lstm_cells = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for lstm in lstms]\n",
    "    #Stack up multiple LSTM layers\n",
    "    stacked_decoder_cells = tf.contrib.rnn.MultiRNNCell(decoder_lstm_cells)\n",
    "    \n",
    "    #Build Attention cells\n",
    "    #Attention Mechanisms\n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units = num_hidden*2, memory = encoder_outputs, memory_sequence_length = inputs_length, name='BahdanauAttention')\n",
    "    # Attention Wrapper\n",
    "    attention_cell = tf.contrib.seq2seq.AttentionWrapper(cell = stacked_decoder_cells,attention_mechanism = attention_mechanism,attention_layer_size = num_hidden*2, name=\"attention_wrapper\")  \n",
    "    #Pass the encoder states to attention\n",
    "    initial_state_attention = attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(cell_state=encoder_state)\n",
    "    \n",
    "    return initial_state_attention,attention_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data for train and test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96516"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the data into train, test \n",
    "X_train, X_test, y_train, y_test = train_test_split(data_sequences, data_seq_subj, test_size=0.1, random_state=1)\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(embeds_input,num_hidden,lstm_layer_numbers,keep_prob,batch_size,inputs_length):\n",
    "    '''Create Bidirectional encoder to make model more precise '''\n",
    "    \n",
    "    lstm_cells_encoder_fw, lstm_cells_encoder_bw = build_RNN_encoder_cells(num_hidden,lstm_layer_numbers,keep_prob,batch_size)\n",
    "    #Need to unstack the sequence of input into a list of tensors\n",
    "    #seq_input = [tf.squeeze(i,[1]) for i in tf.split(embeds_input,input_len,1)] \n",
    "    \n",
    "    #Decoder needs ecnoders's final states as its initial state (It will pass through attention)\n",
    "    (encoder_fw_output,encoder_bw_output),(encoder_fw_state,encoder_bw_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_cells_encoder_fw, cell_bw=lstm_cells_encoder_bw, inputs=embeds_input,sequence_length=inputs_length, dtype=tf.float32)\n",
    "    #Concat the backward forward outputs from encoder\n",
    "    encoder_output = tf.concat((encoder_fw_output, encoder_bw_output),2)\n",
    "    #Concat the backward forward states from encoder\n",
    "    encoder_states = []\n",
    "    for i in range(lstm_layer_numbers):\n",
    "        #Basic LSTM state is a tuple contains cell and hidden states \n",
    "        state_c = tf.concat(values=(encoder_fw_state[i].c,encoder_bw_state[i].c),axis=1)\n",
    "        state_h = tf.concat(values=(encoder_fw_state[i].h,encoder_bw_state[i].h),axis=1)\n",
    "        encoder_state = tf.contrib.rnn.LSTMStateTuple(c=state_c, h=state_h)\n",
    "        encoder_states.append(encoder_state)\n",
    "    encoder_states = tuple(encoder_states)\n",
    "    #From the tuple get the state part not the output\n",
    "    encoder_states_c = encoder_states\n",
    "    \n",
    "    return encoder_output,encoder_states_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference(prediction) has different decoder from traning as expained very well by https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(embeds_target,encoder_output,encoder_states_c,num_hidden,lstm_layer_numbers,batch_size,inputs_length,target_length):\n",
    "    '''Create decoder with attention '''\n",
    "\n",
    "    initial_state,lstm_cells_decoder=build_RNN_decoder_cells(encoder_output,encoder_states_c,num_hidden,lstm_layer_numbers,batch_size,inputs_length)\n",
    "\n",
    "    #Decoder setup\n",
    "    \n",
    "    #Helps to convert outputs to logits\n",
    "    output_layer = Dense(vocab_size)\n",
    "    \n",
    "    #Training\n",
    "    #Training helper (helper is used for BasicDecoder)\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(embeds_target,sequence_length=target_length,time_major=False)\n",
    "    #Training decoder\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(lstm_cells_decoder, helper, initial_state,output_layer = output_layer)\n",
    "    print(embeds_target.shape)\n",
    "    print(target_length.shape)\n",
    "    train_final_outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(train_decoder,output_time_major=False,impute_finished=True)\n",
    "\n",
    "    #Inference\n",
    "    #need for GreedyEmbeddingHelper parameter\n",
    "    start_tokens = tf.fill([batch_size], word_index[\"<SOS>\"])\n",
    "    #Inference helper (helper is used for BasicDecoder)\n",
    "    #For inference we use GreedyEmbeddingHelper because the ground truth is not available as input and it uses the output of the previous timestep instead (First param is embeddings vector)\n",
    "    helper_inf = tf.contrib.seq2seq.GreedyEmbeddingHelper(emneddings,start_tokens,word_index[\"<EOS>\"])\n",
    "    #Inference decoder\n",
    "    inf_decoder = tf.contrib.seq2seq.BasicDecoder(lstm_cells_decoder, helper_inf, initial_state,output_layer = output_layer)\n",
    "    \n",
    "    inf_final_outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(inf_decoder,output_time_major=False,impute_finished=True)\n",
    "\n",
    "\n",
    "    logits_train = train_final_outputs.rnn_output\n",
    "    logits_inf = inf_final_outputs.rnn_output\n",
    "    \n",
    "    return logits_train,logits_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary size plus one for 0, the int number that added for padding (still need to think about this!?)\n",
    "vocab_size = len(word_index)+1\n",
    "#Number of units\n",
    "num_hidden = 256\n",
    "#Encoder and decoder layers \n",
    "lstm_layer_numbers=2\n",
    "#Encoder and decoder embedding size \n",
    "embed_size=300\n",
    "#To avoid ResourceExhaustedError due to batch size use a proper size based on gpu performance (still OOM problem)\n",
    "batch_size= 24\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding and decoding layers are created. From encoding layers, only the encoding's output state is needed for decoding layer as its intial state.<br> If one uses attention, they also need to use the encode's output for attention cell, otherwise, encode's output is not needed.\n",
    "We need to create the decoder for both training and inference(prediction) phases.<br> \n",
    "The difference is that in training decoder, the inputs of decoder are the sequences of targets that are fed to the decoder to create the model but in inference(prediction) phase the decoder works like language model in which the output of the previous time step in decoder is fed to the input of the next time step in decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(184972, 300) dtype=float32_ref>\n",
      "Encoder\n",
      "Decoder\n",
      "(24, ?, 300)\n",
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "#Resert the default graph \n",
    "tf.reset_default_graph()\n",
    "graph0 = tf.Graph()\n",
    "#There exits a global default graph created by tensorflow, for new graphs we need to set them as a default graph\n",
    "with graph0.as_default():\n",
    "    encoding_inputs,decoder_targets,keep_prob,inputs_length,target_length=create_model_inputs()\n",
    "    #tf.AUTO_REUSE for reuisng the same scope for generating as for training\n",
    "    #with tf.variable_scope('rnn1', reuse=tf.AUTO_REUSE):\n",
    "    #Create word embeddings from words using Glove\n",
    "    emneddings=build_embeddings(vocab_size,embed_size,word_index,words_in_glove)\n",
    "    \n",
    "    print(\"Encoder\")\n",
    "    #ENCODER\n",
    "    \n",
    "    #Create embeddings for encoding_inputs (Encoder)\n",
    "    embeds_input = tf.nn.embedding_lookup(emneddings, encoding_inputs)\n",
    "    \n",
    "    encoder_output,encoder_states_c = build_encoder(embeds_input,num_hidden,lstm_layer_numbers,keep_prob,batch_size,inputs_length)\n",
    "\n",
    "    print(\"Decoder\")\n",
    "    #DECODER\n",
    "    \n",
    "    #As mentioned here: https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb\n",
    "    #We should remove the last words from each target sequence in the decoder since in the last time step in decoder the target input is the last word from the sequence target and it will be ignored (This word is either <PAD> or <EOS>)\n",
    "    sliced_targets= tf.strided_slice(decoder_targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "    #Need to append SOS to each target sequenece\n",
    "    decoder_targets = tf.concat([tf.fill([batch_size, 1], word_index[\"<SOS>\"]), sliced_targets], 1)\n",
    "    #Create embeddings for targets (Decoder)\n",
    "    embeds_target = tf.nn.embedding_lookup(emneddings, decoder_targets)\n",
    "    \n",
    "    logits_train,logits_inf = build_decoder(embeds_target,encoder_output,encoder_states_c,num_hidden,lstm_layer_numbers,batch_size,inputs_length,target_length)\n",
    "\n",
    "    \n",
    "    masks = tf.sequence_mask(target_length,target_len, dtype=tf.float32)\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logits_train,decoder_targets,masks)\n",
    "\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss=loss,global_step=tf.train.get_global_step(),optimizer=tf.train.AdamOptimizer,learning_rate=learning_rate)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the graph for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Execute the graph for training\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "with tf.Session(graph=graph0,config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    sess = tf.Session(graph=graph0)\n",
    "    sess.run(init_op)\n",
    "    no_of_batches_train = int(len(X_train)/batch_size)\n",
    "    epochs = 20\n",
    "    text_len=[input_len] * batch_size\n",
    "    text_len = np.asarray(text_len)\n",
    "    summary_len=[target_len] * batch_size\n",
    "    summary_len = np.asarray(summary_len)\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        state = sess.run(init_op)\n",
    "        avg_cost_train = 0 \n",
    "        for ii, (x, y) in enumerate(get_batches(X_train, y_train, batch_size), 1):\n",
    "            _, cost= sess.run([train_op, loss], feed_dict={encoding_inputs: x,\n",
    "                                                            decoder_targets: y,keep_prob: 0.5,inputs_length:text_len,target_length:summary_len})\n",
    "\n",
    "            avg_cost_train += cost / no_of_batches_train\n",
    "        print(\"Epoch:\", epoch+1, \"cost_train=\",avg_cost_train)\n",
    "    #Save the model into a file \n",
    "    checkpoint=\"./model/savedmodel.ckpt\"\n",
    "    save_path = saver.save(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict email subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute the same graph0 for prediction\n",
    "with tf.Session(graph=graph0) as sess2:\n",
    "    # Load the model\n",
    "    saved = tf.train.import_meta_graph('./model/savedmodel.ckpt.meta')\n",
    "    saved.restore(sess2, checkpoint)\n",
    "    \n",
    "    input_test=np.asarray(batch_size* [X_train[0]])\n",
    "    print(input_test.shape)\n",
    "    prediction= sess2.run([logits_inf ], feed_dict={encoding_inputs: input_test,\n",
    "                                                            keep_prob: 0.5,inputs_length:text_len,target_length:summary_len})\n",
    "Subject_Email=[]\n",
    "for i in prediction:\n",
    "    Subject_Email.append(get_by_key_dict(word_int,words_index))\n",
    "print(\"Email Content:\",X_train[0])\n",
    "print(\"Predicted Subject:\",Subject_Email)\n",
    "print(\"real Subject:\",y_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working to solve the problem:<br> There is a OOM(out of memory) problem even when running with a good gpu and reducing the batch_size OOM remains. Since this happens after training step, I belive the memory cannot be released from training step and it runs out of memory in prediction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
