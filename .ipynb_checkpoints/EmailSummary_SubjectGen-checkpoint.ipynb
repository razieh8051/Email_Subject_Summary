{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a seq_to_seq problem (needs encoder and decoder)<br>\n",
    "Process:<br>\n",
    "1-Load the data <br>\n",
    "2-Preprocess the data <br>\n",
    "3-Encode the sentences (create dictionary from the words, map words to integers) <br>\n",
    "4-Build and train the seq2seq model (Using GloVe for the embeddings and Attention with decoder) <br> \n",
    "5-Generate the summary <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/inspect.py\", line 1051, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/inspect.py\", line 1011, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/inspect.py\", line 453, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/inspect.py\", line 496, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/inspect.py\", line 465, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/inspect.py\", line 444, in getsourcefile\n",
      "    if string.lower(filename[-4:]) in ('.pyc', '.pyo'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Users/razieh/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/razieh/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/razieh/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1411\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/razieh/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             )\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/razieh/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 421, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 636, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 661, in _abort_queue\n",
      "    poller.poll(50)\n",
      "  File \"/Users/razieh/anaconda2/lib/python2.7/site-packages/zmq/sugar/poll.py\", line 99, in poll\n",
      "    return zmq_poll(self.sockets, timeout=timeout)\n",
      "  File \"zmq/backend/cython/_poll.pyx\", line 123, in zmq.backend.cython._poll.zmq_poll\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "    PyErr_CheckSignals()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tensorflow.python.layers.core import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from Enron email dataset. <br>\n",
    "In this case we consider the subject of an email a few words summary that we need to learn for that email. Therefore, the inputs are email contents and targets are email subjects (or summaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/emails_data/enron_emails.csv')\n",
    "#Neen only two columns 'Subject' and 'content'\n",
    "df1 = df[['Subject','content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We disregard the forwarded and replied emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=df1[~df1['Subject'].str.contains(\"FW:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"Fw:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"fw:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"RE:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"Re:\", na=False)]\n",
    "df1=df1[~df1['Subject'].str.contains(\"re:\", na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emails that contain \"Forwarded by\" in their content are the replied emails that the subjects are changed by the current sender. Therfore, for now we disregard those too (Althoug later we can use the replied emails for test to give them subjects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=df1[~df1['content'].str.contains(\"Forwarded by\", na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NaN subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[pd.notnull(df1['Subject'])]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out the maximum and minimum length for content column so we can define a specific length range for emails that we want to include in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = df1.applymap(lambda x: len(str(x))).max()\n",
    "print(max_len)\n",
    "min_len = df1.applymap(lambda x: len(str(x))).min()\n",
    "print(min_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the email with the content in the range of [500,6000] characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = (0<df1['Subject'].str.len()<258) & (500<df1['content'].str.len() <6000)\n",
    "#df1 = df1.loc[mask]\n",
    "df1=df1[df1['content'].astype('str').map(len) <= 6000]\n",
    "df1=df1[df1['content'].astype('str').map(len) >= 500] \n",
    "emails=df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean(emails,stop_words):\n",
    "    '''Clean the data both subjects and content of emails'''\n",
    "    emails_messages=[]\n",
    "    subj_messages=[]\n",
    "    for email_content in emails['content']:\n",
    "        #Extra celaning of text before Keras tokenization \n",
    "        #Removing stopwords                \n",
    "        email_content=' '.join(i for i in email_content.split() if i not in stop_words)\n",
    "        #Removing special characters and float numbers\n",
    "        email_content=re.sub(\"(\\d*\\.\\d+)|(\\d+\\.[0-9 ]+)\",\"\",email_content)\n",
    "        email_content=re.sub(r'[^\\w]', ' ', email_content)\n",
    "        '''for word in email_content:\n",
    "            email_content=\" \".join([w for w in email_content.split() if not w.isdigit()])'''\n",
    "        #remove all numbers (except for joint numbers to strings such as 27th; we also may later try to keep numbers related to dates and rooms, money , etc such as Sep 27, room numbers 3, 10 cent, etc)\n",
    "        email_content = \" \".join([w for w in email_content.split() if not w.isdigit()])\n",
    "\n",
    "        emails_messages.append(email_content)\n",
    "    for subject_messages in emails['Subject']:\n",
    "        #Extra celaning of text before Keras tokenization \n",
    "        #Removing stopwords                \n",
    "        subject_messages=' '.join(i for i in subject_messages.split() if i not in stop_words)\n",
    "        #Removing special characters and float numbers\n",
    "        subject_messages=re.sub(\"(\\d*\\.\\d+)|(\\d+\\.[0-9 ]+)\",\"\",subject_messages)\n",
    "        subject_messages=re.sub(r'[^\\w]', ' ', subject_messages)\n",
    "        '''for word in email_content:\n",
    "            email_content=\" \".join([w for w in email_content.split() if not w.isdigit()])'''\n",
    "        #remove all numbers (except for joint numbers to strings such as 27th; we also may later try to keep numbers related to dates and rooms, money , etc such as Sep 27, room numbers 3, 10 cent, etc)\n",
    "        subject_messages = \" \".join([w for w in subject_messages.split() if not w.isdigit()])\n",
    "\n",
    "        subj_messages.append(subject_messages)    \n",
    "\n",
    "    return subj_messages,emails_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stop words from nltk\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#clean and preprocess the data\n",
    "subject_messages,emails_messages=load_clean(emails,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comapring the first email before and after perprocessing\n",
    "print(\"First email before:\",emails.loc[24,'content'])\n",
    "\n",
    "print(\"First email after:\",emails_messages[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words_in_seqences(phrases,sentences):\n",
    "    '''Convert words to numbers (Create dictionary of words)'''\n",
    "    \n",
    "    #Get all words in sentences(content) and phrases(subjects)\n",
    "    word_list_content = ' '.join(sentences).split(' ')   \n",
    "    word_list_subj = ' '.join(phrases).split(' ')   \n",
    "    word_list = word_list_content + word_list_subj\n",
    "    word_list=set(word_list)\n",
    "    #number of unique words in all above\n",
    "    text_len=len(word_list)\n",
    "    \n",
    "    #Initial different sequences for list of contents and list of subjects\n",
    "    data_seq=[]\n",
    "    data_seq_subj=[]\n",
    "    \n",
    "    word_index=dictionary(word_list)\n",
    "    #add EOS and SOS to dictionary (only for decoder,i.e.,subjects not encoder)\n",
    "    word_index[\"<SOS>\"]=text_len+1\n",
    "    word_index[\"<EOS>\"]=text_len+2\n",
    "    word_index[\"<PAD>\"]=text_len+3\n",
    "    for s in sentences:\n",
    "            s=s.split(\" \")\n",
    "            s=[word_index[w] for w in s]\n",
    "            data_seq.append(s)\n",
    "    for p in phrases:\n",
    "            p=p.split(\" \")\n",
    "            #add SOS and EOS to subjects because we only need it for decoder \n",
    "            p.insert(0,\"<SOS>\")\n",
    "            p.insert(len(p)+1,\"<EOS>\")\n",
    "            p=[word_index[w] for w in p]\n",
    "            data_seq_subj.append(p)\n",
    "    \n",
    "    #Choose the maximum number of tokens in all sequences \n",
    "    num_tokens = [len(tokens) for tokens in data_seq]\n",
    "    max_seq_length=np.max(num_tokens)\n",
    "    \n",
    "    num_tokens_subj = [len(tokens) for tokens in data_seq_subj]\n",
    "    max_seq_subj_length=np.max(num_tokens_subj)\n",
    "    \n",
    "    #Padding sequences\n",
    "    #Make sequences to have the same lengths (add extra zeros to the end of the sentences)\n",
    "    #PAD's value is word_index[\"<PAD>\"]=text_len+3 \n",
    "    data_seq = pad_sequences(data_seq, maxlen = max_seq_length,\n",
    "                                padding='post', truncating='pre', value=word_index[\"<PAD>\"])\n",
    "    data_seq_subj = pad_sequences(data_seq_subj, maxlen = max_seq_subj_length,\n",
    "                                padding='post', truncating='pre', value=word_index[\"<PAD>\"])\n",
    "    \n",
    "    return word_index,data_seq,data_seq_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary from list of words in text\n",
    "def dictionary(words):\n",
    "    #create list of words without their dupications \n",
    "    words=set(words)\n",
    "    #map word to index\n",
    "    indx = {key: i for i, key in enumerate(words)}\n",
    "    return indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from index to words\n",
    "def get_by_key_dict(indx_word,words_dict):\n",
    "    for word, indx in words_dict.iteritems():    \n",
    "        if indx == indx_word:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words to int\n",
    "word_index,data_sequences,data_seq_subj=encode_words_in_seqences(subject_messages,emails_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences for subjects\n",
    "data_seq_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences for contents\n",
    "data_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the input and target lengths\n",
    "input_len=len(data_sequences[0])\n",
    "target_len=len(data_seq_subj[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the naive approach for embedding (which is initializing the embedding vectors with random numbers and then let our model to further learn the embeddings) we can use GloVe to initialize some of the embeddings with pre_trained data learned from GloVe and initialize the nn existing words by GloVe with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glovefile=\"/glovedata/glove.6B.50d.txt\"\n",
    "#Load Glove model from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "def loadGloveModel(gloveFile):\n",
    "    words_in_glove=[]\n",
    "    print \"Loading Glove Model\"\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        words_in_glove.append(word)\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model,words_in_glove\n",
    "model,words_in_glove=loadGloveModel(glovefile)\n",
    "\n",
    "print model['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings of GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(vocabulary_size,embedding_size,word_index,words_in_glove):\n",
    "    embedding_all= tf.Variable(tf.random_uniform((vocabulary_size, embedding_size), -1, 1))\n",
    "    #For the words in the emails that are not in the glove we give them random embeddings\n",
    "    '''for i in range(0,vocabulary_size):\n",
    "        if get_by_key_dict(i,word_index) in words_in_glove:\n",
    "            tf.assign(embedding_all[i],model[get_by_key_dict(i,word_index)])'''\n",
    "    print(embedding_all)\n",
    "    return embedding_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build encoder cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN_encoder_cells(num_hidden,lstm_layer_numbers,keep_prob,batch_size):\n",
    "    '''Build RNN encoder cells'''\n",
    "\n",
    "    #Define LSTM layers(Bidirectional need both backward and forward info)\n",
    "    lstms=[]\n",
    "    for i in range(lstm_layer_numbers):\n",
    "        lstms.append(tf.contrib.rnn.BasicLSTMCell(num_hidden))\n",
    "    # Add regularization dropout to the LSTM cells\n",
    "    lstm_fw_cell = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for lstm in lstms]\n",
    "    lstm_bw_cell = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for lstm in lstms]\n",
    "\n",
    "    # Stack up multiple LSTM layers\n",
    "    stacked_lstm_fw = tf.contrib.rnn.MultiRNNCell(lstm_fw_cell)\n",
    "    stacked_lstm_bw = tf.contrib.rnn.MultiRNNCell(lstm_bw_cell)\n",
    "    \n",
    "    return stacked_lstm_fw,stacked_lstm_bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build decoder cells (with attention) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN_decoder_cells(encoder_outputs,encoder_state,num_hidden,lstm_layer_numbers,batch_size,inputs_length):\n",
    "    '''Build RNN decoder attention cells'''\n",
    "    #Define LSTM layers\n",
    "    lstms=[]\n",
    "    for i in range(lstm_layer_numbers):\n",
    "        lstms.append(tf.contrib.rnn.BasicLSTMCell(num_hidden*2))\n",
    "    # Add regularization dropout to the LSTM cells\n",
    "    decoder_lstm_cells = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for lstm in lstms]\n",
    "    # Stack up multiple LSTM layers\n",
    "    stacked_decoder_cells = tf.contrib.rnn.MultiRNNCell(decoder_lstm_cells)\n",
    "    \n",
    "    #Build Attention cells\n",
    "    # Attention Mechanisms\n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units = num_hidden*2, memory = encoder_outputs, memory_sequence_length = inputs_length, name='BahdanauAttention')\n",
    "    # Attention Wrapper\n",
    "    attention_cell = tf.contrib.seq2seq.AttentionWrapper(cell = stacked_decoder_cells,attention_mechanism = attention_mechanism,attention_layer_size = num_hidden*2, name=\"attention_wrapper\")  \n",
    "    #Pass the encoder states to attention\n",
    "    initial_state_attention = attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(cell_state=encoder_state)\n",
    "    return initial_state_attention,attention_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    '''Using generator to return batches for train, validation and test data'''\n",
    "\n",
    "    n_batches = len(x)//batch_size\n",
    "    '''In case that the batch_size is not a multiple of data size in order to create batches with the same sizes, this line will ignore the last words in text that cannot create a full batch (although, one can consider those last words and add enough words from the beginning of the text to create a full size batch)'''\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  create_model_inputs():\n",
    "    '''Define model inputs'''\n",
    "    \n",
    "    #Model's placeholders for inputs\n",
    "    encoding_inputs = tf.placeholder(tf.int32, [None, None], name='encode_inputs')\n",
    "    decoder_targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='inputs_length')\n",
    "    target_length =tf.placeholder(shape=(None,), dtype=tf.int32, name='target_length')\n",
    "\n",
    "    return encoding_inputs,decoder_targets,keep_prob,inputs_length,target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data for train and test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train,test \n",
    "X_train, X_test, y_train, y_test = train_test_split(data_sequences, data_seq_subj, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary size plus one for 0, the int number that added for padding\n",
    "vocab_size = len(word_index)\n",
    "#number of units\n",
    "num_hidden = 256\n",
    "#encoder and decoder layers \n",
    "lstm_layer_numbers=2\n",
    "#encoder and decoder embedding size \n",
    "embed_size=300\n",
    "batch_size= 250\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encdoing and decoding layers are created and from encoding layers only the encoding output state is needed for decoding layer as its intial state.<br> If one uses attention they also need to use the encode's output for attention cell, otherwise, encode's output is not needed.\n",
    "We need to create decoder for both training and inference(prediction) phases.<br> \n",
    "The difference is that in training decoder, the inputs of decoder are the sequences of targets that are fed to the decoder to create the model but in inference(prediction) phase the decoder works like language model in which the output of the previous time step in decoder is fed to the input of the next time step in decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resert the default graph \n",
    "tf.reset_default_graph()\n",
    "graph0 = tf.Graph()\n",
    "#There exits a global default graph created by tenserflow, for new graphs we need to set them as a default graph\n",
    "with graph0.as_default():\n",
    "    encoding_inputs,decoding_inputs,keep_prob,inputs_length,target_length=create_model_inputs()\n",
    "    #tf.AUTO_REUSE for reuisng the same scope for generating as for traning\n",
    "    #with tf.variable_scope('rnn1', reuse=tf.AUTO_REUSE):\n",
    "    #Create word embeddings from words using Glove\n",
    "    emneddings=build_embeddings(vocab_size,embed_size,word_index,words_in_glove)\n",
    "    #Create embeddings for encoding_inputs\n",
    "    embeds_input = tf.nn.embedding_lookup(emneddings, encoding_inputs)\n",
    "\n",
    "    print(\"Encoder\")\n",
    "    #ENCODER\n",
    "    #Bi directional encoder is used to make model more precise \n",
    "    lstm_cells_encoder_fw, lstm_cells_encoder_bw = build_RNN_encoder_cells(num_hidden,lstm_layer_numbers,keep_prob,batch_size)\n",
    "    #need to unstack the sequence of input into a list of tensors\n",
    "    #seq_input = [tf.squeeze(i,[1]) for i in tf.split(embeds_input,input_len,1)] \n",
    "    \n",
    "    #decoder needs ecnoders's final states as its initial state (It will pass through attention)\n",
    "    (encoder_fw_output,encoder_bw_output),(encoder_fw_state,encoder_bw_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_cells_encoder_fw, cell_bw=lstm_cells_encoder_bw, inputs=embeds_input,sequence_length=inputs_length, dtype=tf.float32)\n",
    "    #concat the backward forward outputs from encoder\n",
    "    encoder_output = tf.concat((encoder_fw_output, encoder_bw_output),2)\n",
    "    #concat the backward forward states from encoder\n",
    "    encoder_states = []\n",
    "    for i in range(lstm_layer_numbers):\n",
    "        #Basic LSTM state is a tuple contains cell and hidden states \n",
    "        state_c = tf.concat(values=(encoder_fw_state[i].c,encoder_bw_state[i].c),axis=1,name=\"encoder_fw_state_c\")\n",
    "        state_h = tf.concat(values=(encoder_fw_state[i].h,encoder_bw_state[i].h),axis=1,name=\"encoder_fw_state_h\")\n",
    "        encoder_state = tf.contrib.rnn.LSTMStateTuple(c=state_c, h=state_h)\n",
    "        encoder_states.append(encoder_state)\n",
    "    encoder_states = tuple(encoder_states)\n",
    "    #From tuple get the state part not the output\n",
    "    encoder_states_c = encoder_states\n",
    "\n",
    "    print(\"Decoder\")\n",
    "    #DECODER\n",
    "    #As mentioned here: https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb\n",
    "    #We should remove the last words from each target sequence in decoder since in the last time setp in decoder the target input is the last word from the sequence target and it will be ignored (This word is either <PAD> or <EOS>)\n",
    "    sliced_targets= tf.strided_slice(decoding_inputs, [0, 0], [batch_size, -1], [1, 1])\n",
    "    #Need to append SOS to each target sequenece\n",
    "    decoding_inputs = tf.concat([tf.fill([batch_size, 1], word_index[\"<SOS>\"]), sliced_targets], 1)\n",
    "    #Create embeddings for decoding_inputs\n",
    "    embeds_target = tf.nn.embedding_lookup(emneddings, decoding_inputs)\n",
    "    #decoder with attention \n",
    "    initial_state,lstm_cells_decoder=build_RNN_decoder_cells(encoder_output,encoder_states_c,num_hidden,lstm_layer_numbers,batch_size,inputs_length)\n",
    "\n",
    "    #Decoder setup\n",
    "    \n",
    "    #Helps to convert outputs to logits\n",
    "    output_layer = Dense(vocab_size)\n",
    "    \n",
    "    #Training\n",
    "    #Training helper (helper is used for BasicDecoder)\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(embeds_target,sequence_length=target_length,time_major=False)\n",
    "    #Training decoder\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(lstm_cells_decoder, helper, initial_state,output_layer = output_layer)\n",
    "    print(embeds_target.shape)\n",
    "    print(target_length.shape)\n",
    "    train_outputs = tf.contrib.seq2seq.dynamic_decode(train_decoder,output_time_major=False,impute_finished=True)\n",
    "\n",
    "    #Inference\n",
    "    #need for GreedyEmbeddingHelper parameter\n",
    "    start_tokens = tf.fill([batch_size], word_index[\"<SOS>\"])\n",
    "    #Inference helper (helper is used for BasicDecoder)\n",
    "    #For inference we use GreedyEmbeddingHelper because the ground truth is not available as input and it uses the output of the previous timestep instead\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeds_target,start_tokens,word_index[\"<EOS>\"])\n",
    "    #Inference decoder\n",
    "    inf_decoder = tf.contrib.seq2seq.BasicDecoder(lstm_cells_decoder, helper, initial_state,output_layer = output_layer)\n",
    "\n",
    "    inf_outputs = tf.contrib.seq2seq.dynamic_decode(inf_decoder)\n",
    "\n",
    "\n",
    "    logits_train = train_outputs.rnn_output\n",
    "    logits_inf = inf_outputs.rnn_output\n",
    "\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logits_train,targets=target_seq, weights=tf.ones(trg_batch.shape))\n",
    "\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss=loss,global_step=tf.contrib.framework.get_global_step(),optimizer=tf.train.AdamOptimizer,learning_rate=0.001)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the graph for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute the graph for training\n",
    "with tf.Session(graph=graph0) as sess:\n",
    "    sess = tf.Session(graph=graph0)\n",
    "    sess.run(init_op)\n",
    "    no_of_batches_train = int(len(X_train)/batch_size)\n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        state = sess.run(initial_state_train)\n",
    "        avg_cost_train = 0 \n",
    "        avg_acc_train= 0\n",
    "        for ii, (x, y) in enumerate(get_batches(X_train, y_train, batch_size), 1):\n",
    "            _, cost= sess.run([optimizer, loss], feed_dict={encoding_inputs: x,\n",
    "                                                            decoder_targets: y[:, None],keep_prob: 0.5,inputs_length:input_len,target_length:target_len})\n",
    "\n",
    "            avg_cost_train += cost / no_of_batches_train\n",
    "            avg_acc_train += acc / no_of_batches_train\n",
    "        print(\"Epoch:\", epoch+1, \"cost_train=\")\n",
    "        print(\"acc_train=\", avg_acc_train) \n",
    "    #Save the model into a file \n",
    "    checkpoint=\"./model/savedmodel.ckpt\"\n",
    "    save_path = saver.save(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict email subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Execute the same graph0 for prediction\n",
    "with tf.Session(graph=graph0) as sess2:\n",
    "    # Load the model\n",
    "    saved = tf.train.import_meta_graph('./model/savedmodel.ckpt.meta')\n",
    "    saved.restore(sess2, checkpoint)\n",
    "    prediction= sess2.run([logits_inf ], feed_dict={inputs: X_test,\n",
    "                                                                    keep_prob: 0.5})\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
